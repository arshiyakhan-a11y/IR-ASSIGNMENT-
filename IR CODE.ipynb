{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install all required packages\n",
        "!pip install scikit-learn rank-bm25 nltk pandas joblib tqdm chardet --quiet\n",
        "\n",
        "print(\"Packages installed\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SlWLjRWDROXO",
        "outputId": "9fe17ad9-48a2-419a-efe8-279e9ddf7ee9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Packages installed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "# Download ALL required NLTK data\n",
        "print(\"Downloading NLTK language data...\")\n",
        "print(\"This may take a minute...\")\n",
        "\n",
        "# Download everything we need\n",
        "nltk.download('punkt', quiet=False)\n",
        "nltk.download('punkt_tab', quiet=False)  # This is what's missing\n",
        "nltk.download('stopwords', quiet=False)\n",
        "nltk.download('averaged_perceptron_tagger', quiet=False)\n",
        "nltk.download('wordnet', quiet=False)\n",
        "\n",
        "print(\"All NLTK data downloaded successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wQ5SBAkORSbI",
        "outputId": "5dadb694-a947-47ae-e1c6-19c591bb4bee"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading NLTK language data...\n",
            "This may take a minute...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All NLTK data downloaded successfully!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Check if file already exists\n",
        "if not os.path.exists(\"Articles.csv\"):\n",
        "    print(\"Please upload your Articles.csv file\")\n",
        "    uploaded = files.upload()\n",
        "else:\n",
        "    print(\"Articles.csv already exists\")\n",
        "    print(f\"File size: {os.path.getsize('Articles.csv')} bytes\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "li38d2NwRbBI",
        "outputId": "c3d30f3f-a160-4e72-91af-d5d0a2a6d4dc"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Articles.csv already exists\n",
            "File size: 5071129 bytes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import chardet\n",
        "\n",
        "print(\"Checking file encoding...\")\n",
        "with open('Articles.csv', 'rb') as f:\n",
        "    raw_data = f.read(100000)\n",
        "    result = chardet.detect(raw_data)\n",
        "    print(f\" Detected encoding: {result['encoding']} ({result['confidence']:.2%} confidence)\")\n",
        "\n",
        "# Test reading\n",
        "try:\n",
        "    df = pd.read_csv(\"Articles.csv\", encoding=result['encoding'])\n",
        "    print(f\" File read successfully!\")\n",
        "    print(f\" Rows: {len(df)}, Columns: {df.columns.tolist()}\")\n",
        "    print(\"\\n Sample:\")\n",
        "    print(df.head(2))\n",
        "    working_encoding = result['encoding']\n",
        "except:\n",
        "    print(\"Trying alternative encodings...\")\n",
        "    for enc in ['latin-1', 'ISO-8859-1', 'cp1252', 'utf-8', 'utf-16']:\n",
        "        try:\n",
        "            df = pd.read_csv(\"Articles.csv\", encoding=enc)\n",
        "            print(f\"Works with: {enc}\")\n",
        "            working_encoding = enc\n",
        "            break\n",
        "        except:\n",
        "            continue\n",
        "\n",
        "print(f\"\\n Use this encoding in next step: '{working_encoding}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rP4YrYN4Ribz",
        "outputId": "f725c733-05df-4501-e4bc-78543909c78b"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking file encoding...\n",
            " Detected encoding: Windows-1252 (73.00% confidence)\n",
            " File read successfully!\n",
            " Rows: 2692, Columns: ['Article', 'Date', 'Heading', 'NewsType']\n",
            "\n",
            " Sample:\n",
            "                                             Article      Date  \\\n",
            "0  KARACHI: The Sindh government has decided to b...  1/1/2015   \n",
            "1  HONG KONG: Asian markets started 2015 on an up...  1/2/2015   \n",
            "\n",
            "                                             Heading  NewsType  \n",
            "0  sindh govt decides to cut public transport far...  business  \n",
            "1                    asia stocks up in new year trad  business  \n",
            "\n",
            " Use this encoding in next step: 'Windows-1252'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import joblib\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Make sure NLTK data is available\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    nltk.data.find('tokenizers/punkt_tab/english')\n",
        "except LookupError:\n",
        "    print(\" Downloading missing NLTK data...\")\n",
        "    nltk.download('punkt', quiet=True)\n",
        "    nltk.download('punkt_tab', quiet=True)\n",
        "    nltk.download('stopwords', quiet=True)\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Text cleaning functions\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text.strip()\n",
        "\n",
        "def tokenize_text(text):\n",
        "    words = word_tokenize(text)\n",
        "    words = [w for w in words if re.search(r\"[a-zA-Z0-9]\", w)]\n",
        "    words = [w for w in words if w not in stop_words]\n",
        "    return words\n",
        "\n",
        "# Search System Class\n",
        "class ArticleSearch:\n",
        "    def __init__(self, documents, doc_ids=None):\n",
        "        self.documents = documents\n",
        "        self.doc_ids = doc_ids or list(range(len(documents)))\n",
        "        self.clean_docs = [clean_text(d) for d in documents]\n",
        "        self.tokenized_docs = [tokenize_text(d) for d in self.clean_docs]\n",
        "        self.tfidf_vectorizer = None\n",
        "        self.tfidf_matrix = None\n",
        "        self.bm25_index = None\n",
        "\n",
        "    def create_tfidf(self):\n",
        "        self.tfidf_vectorizer = TfidfVectorizer()\n",
        "        self.tfidf_matrix = self.tfidf_vectorizer.fit_transform(self.clean_docs)\n",
        "\n",
        "    def create_bm25(self):\n",
        "        self.bm25_index = BM25Okapi(self.tokenized_docs)\n",
        "\n",
        "    def save_index(self, filename):\n",
        "        joblib.dump({\n",
        "            'documents': self.documents,\n",
        "            'doc_ids': self.doc_ids,\n",
        "            'clean_docs': self.clean_docs,\n",
        "            'tokenized_docs': self.tokenized_docs,\n",
        "            'tfidf_vectorizer': self.tfidf_vectorizer,\n",
        "            'tfidf_matrix': self.tfidf_matrix,\n",
        "            'bm25_index': self.bm25_index\n",
        "        }, filename)\n",
        "        print(f\" Index saved as {filename}\")\n",
        "\n",
        "    @classmethod\n",
        "    def load_index(cls, filename):\n",
        "        data = joblib.load(filename)\n",
        "        system = cls(data['documents'], data['doc_ids'])\n",
        "        system.clean_docs = data['clean_docs']\n",
        "        system.tokenized_docs = data['tokenized_docs']\n",
        "        system.tfidf_vectorizer = data['tfidf_vectorizer']\n",
        "        system.tfidf_matrix = data['tfidf_matrix']\n",
        "        system.bm25_index = data['bm25_index']\n",
        "        return system\n",
        "\n",
        "    def search_bm25(self, query, num_results=10):\n",
        "        query_tokens = tokenize_text(clean_text(query))\n",
        "        scores = self.bm25_index.get_scores(query_tokens)\n",
        "        top_indices = np.argsort(scores)[::-1][:num_results]\n",
        "        results = []\n",
        "        for i in top_indices:\n",
        "            results.append({\n",
        "                'id': self.doc_ids[i],\n",
        "                'score': float(scores[i]),\n",
        "                'content': self.documents[i]\n",
        "            })\n",
        "        return results\n",
        "\n",
        "    def search_tfidf(self, query, num_results=10):\n",
        "        query_vector = self.tfidf_vectorizer.transform([clean_text(query)])\n",
        "        similarities = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
        "        top_indices = np.argsort(similarities)[::-1][:num_results]\n",
        "        results = []\n",
        "        for i in top_indices:\n",
        "            results.append({\n",
        "                'id': self.doc_ids[i],\n",
        "                'score': float(similarities[i]),\n",
        "                'content': self.documents[i]\n",
        "            })\n",
        "        return results\n",
        "\n",
        "# --- MAIN EXECUTION ---\n",
        "print(\" Loading your articles...\")\n",
        "\n",
        "# USE THE ENCODING FROM STEP 5 HERE!\n",
        "# Example: 'latin-1', 'utf-8', 'cp1252', etc.\n",
        "working_encoding = 'latin-1'  #  CHANGE THIS to what worked in Step 5\n",
        "\n",
        "df = pd.read_csv(\"Articles.csv\", encoding=working_encoding)\n",
        "print(f\" Found {len(df)} articles\")\n",
        "\n",
        "# Check columns\n",
        "print(\"\\n Available columns:\")\n",
        "for i, col in enumerate(df.columns):\n",
        "    print(f\"  {i}: {col}\")\n",
        "\n",
        "# Use first column for text (change index if needed)\n",
        "text_column = df.columns[0]\n",
        "print(f\"\\n Using column '{text_column}' for article content\")\n",
        "\n",
        "# Prepare data\n",
        "doc_texts = df[text_column].fillna(\"\").astype(str).tolist()\n",
        "document_ids = [f\"doc_{i}\" for i in range(len(doc_texts))]  # Create IDs\n",
        "\n",
        "print(f\"\\n Loaded {len(doc_texts)} documents\")\n",
        "print(f\" Average document length: {np.mean([len(d) for d in doc_texts]):.0f} characters\")\n",
        "\n",
        "# Build search system\n",
        "print(\"\\n Building search indexes...\")\n",
        "print(\"  Creating BM25 index...\")\n",
        "search_engine = ArticleSearch(doc_texts, document_ids)\n",
        "search_engine.create_bm25()\n",
        "print(\"  Creating TF-IDF index...\")\n",
        "search_engine.create_tfidf()\n",
        "\n",
        "# Save the index\n",
        "search_engine.save_index(\"article_search_index.pkl\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\" SEARCH SYSTEM READY!\")\n",
        "print(\"=\"*50)\n",
        "print(f\" Index saved: article_search_index.pkl\")\n",
        "print(f\" Documents indexed: {len(doc_texts)}\")\n",
        "print(f\" Methods available: BM25 and TF-IDF\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T95atWXRRuwF",
        "outputId": "d83d70cd-edcb-4b33-b612-ce45146489d1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Loading your articles...\n",
            " Found 2692 articles\n",
            "\n",
            " Available columns:\n",
            "  0: Article\n",
            "  1: Date\n",
            "  2: Heading\n",
            "  3: NewsType\n",
            "\n",
            " Using column 'Article' for article content\n",
            "\n",
            " Loaded 2692 documents\n",
            " Average document length: 1810 characters\n",
            "\n",
            " Building search indexes...\n",
            "  Creating BM25 index...\n",
            "  Creating TF-IDF index...\n",
            " Index saved as article_search_index.pkl\n",
            "\n",
            "==================================================\n",
            " SEARCH SYSTEM READY!\n",
            "==================================================\n",
            " Index saved: article_search_index.pkl\n",
            " Documents indexed: 2692\n",
            " Methods available: BM25 and TF-IDF\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "working_encoding = 'latin-1'  #  CHANGE THIS to what worked in Step 5"
      ],
      "metadata": {
        "id": "Kov9Hqs7SNKZ"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick test\n",
        "search_engine = ArticleSearch.load_index(\"article_search_index.pkl\")\n",
        "\n",
        "# Test with a simple query\n",
        "test_queries = [\"technology\", \"science\", \"health\", \"education\"]\n",
        "\n",
        "for query in test_queries:\n",
        "    print(f\"\\n '{query}':\")\n",
        "    results = search_engine.search_bm25(query, num_results=2)\n",
        "    for i, r in enumerate(results, 1):\n",
        "        preview = r['content'][:100].replace('\\n', ' ') + \"...\"\n",
        "        print(f\"   {i}. [ID:{r['id']}] Score:{r['score']:.3f}\")\n",
        "        print(f\"      {preview}\")\n",
        "\n",
        "print(\"\\n System is working!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U1nuiEcLSPYE",
        "outputId": "7d14516c-e584-45f9-ebe1-23d0065a958e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 'technology':\n",
            "   1. [ID:doc_2650] Score:6.902\n",
            "      SAN FRANCISCO: Alphabet on Thursday filed a lawsuit accusing Uber and its self-driving vehicle unit ...\n",
            "   2. [ID:doc_1807] Score:6.705\n",
            "      strong>SYDNEY: Australia has resorted to guided missile technology to reduce injuries to their fast ...\n",
            "\n",
            " 'science':\n",
            "   1. [ID:doc_1666] Score:7.083\n",
            "      strong>MELBOURNE: Australia fast bowler John Hastings has been withdrawn from a one-day tri-series a...\n",
            "   2. [ID:doc_490] Score:7.038\n",
            "      strong>WASHINGTON: Federal Minister for Planning, Ahsan Iqbal on Saturday said the next 10 years wil...\n",
            "\n",
            " 'health':\n",
            "   1. [ID:doc_549] Score:6.792\n",
            "      strong>NEW DELHI :India´s top cigarette maker ITC Ltd, part-owned by British American Tobacco , said...\n",
            "   2. [ID:doc_603] Score:5.923\n",
            "      strong>Karachi: K-Electric has won two national awards in recognition of its commitment to promote a...\n",
            "\n",
            " 'education':\n",
            "   1. [ID:doc_961] Score:7.676\n",
            "      strong>ISLAMABAD: Sindh Chief Minister Syed Murad Ali Shah has said that World Bank has played an im...\n",
            "   2. [ID:doc_2437] Score:6.853\n",
            "      strong>ISLAMABAD: Prime Minister Nawaz Sharif, Wednesday emphasizing the need to fully tap the busin...\n",
            "\n",
            " System is working!\n"
          ]
        }
      ]
    }
  ]
}